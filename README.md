# MNIST Backpropagation from Scratch
**Designed and developed by:** Farnaz Zinnah  

---

## ğŸ”— Project Overview
This project implements a fully connected neural network (MLP) to classify handwritten digits from the MNIST dataset. The implementation follows a **manually coded forward and backward pass** (backpropagation) without using external deep learning frameworks like TensorFlow or PyTorch.  

---

## ğŸ“ **About**
This project is an implementation of backpropagation for educational purposes.  

The model is trained on the **MNIST dataset** with:
- **Input Layer:** 784 neurons (28x28 pixel images flattened)
- **Two Hidden Layers:** 32 neurons each, with **sigmoid** activation
- **Output Layer:** 10 neurons with **softmax** activation

It uses **stochastic gradient descent (SGD)** to update weights after each sample, computing gradients manually for all layers.

---

## ğŸ¯ **Description & Purpose**
âœ” Implements a **Multi-Layer Perceptron (MLP)**  
âœ” Uses **explicit loops** for matrix multiplications  
âœ” **No external ML libraries** for training (only NumPy)  
âœ” Performs training using **sample-by-sample** updates  
âœ” Tracks accuracy improvement across **3 epochs**  

Expected accuracy: **50%-70%** after training.

---

## ğŸš€ **Tech Stack**
- **Python**
- **NumPy**
- **Matplotlib (for visualization)**
- **TensorFlow (only for loading MNIST dataset)**

---

## ğŸ”§ **Features**
âœ… **Manual Forward Pass:** Uses **sigmoid activation** for hidden layers, softmax for the output layer  
âœ… **Manual Backpropagation:** Computes weight & bias gradients using explicit **Jacobian matrices**  
âœ… **Cross-Entropy Loss:** Computes loss between predicted and true labels  
âœ… **Weight Initialization:** Random weights from a normal distribution (`1/sqrt(n)`)  
âœ… **Stochastic Gradient Descent (SGD):** Updates weights after **each sample**  
âœ… **Evaluation:** Prints accuracy and loss after each epoch  
âœ… **No Mini-batching:** Processes **one sample at a time**  

---

## ğŸ“Š **Results & Performance**
- **Initial Accuracy (before training):** ~10% (random guessing)  
- **After 3 epochs:** ~50%-70% accuracy  

Test loss is initially **higher** than train loss due to **overfitting** on training data and lack of regularization.

---

## ğŸ›  **Installation Instructions**
### 1ï¸âƒ£ Clone the Repository:
```bash
git clone https://github.com/YOUR_GITHUB_USERNAME/mnist-backprop-scratch.git
cd mnist-backprop-scratch
